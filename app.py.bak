import os
import logging
from datetime import datetime, timedelta
from threading import Lock # To safely update the global state
from flask import Flask, render_template, request, redirect, url_for, flash
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.interval import IntervalTrigger
import pytz # For timezone handling with APScheduler

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

import db
import analyzer

# --- Configuration ---
SCAN_INTERVAL_MINUTES = int(os.environ.get("SCAN_INTERVAL_MINUTES", "10"))
OLLAMA_API_URL = analyzer.OLLAMA_API_URL
OLLAMA_MODEL = analyzer.OLLAMA_MODEL
LOG_LINES_TO_FETCH = analyzer.LOG_LINES_TO_FETCH
# Set timezone for scheduler (change if needed, reads TZ env var or defaults to UTC)
SCHEDULER_TIMEZONE = os.environ.get("TZ", "UTC")


app = Flask(__name__)
app.secret_key = os.environ.get("FLASK_SECRET_KEY", "a_default_secret_key_change_me")

# --- Global State for Live Dashboard ---
# Holds the status of all containers found in the *last* scan
# Structure: {container_id: {'name': str, 'status': 'healthy'|'unhealthy'|'pending', 'details': dict|None, 'db_id': int|None}}
container_statuses = {}
container_statuses_lock = Lock() # Protect access from web requests and background thread

scan_status = {
    "last_run_time": None,
    "last_run_status": "Not run yet",
    "next_run_time": None,
    "running": False
}

# --- Background Scanning Task ---
def scan_docker_logs():
    """Scans all running containers, updates in-memory state, logs abnormalities to DB."""
    global container_statuses # We will replace this global dict

    if scan_status["running"]:
        logging.warning("Scan task still running, skipping this interval.")
        return

    scan_status["running"] = True
    start_time = datetime.now()
    scan_status["last_run_time"] = start_time
    logging.info("Starting Docker log scan...")

    current_scan_results = {} # Temporary dict for this scan's findings
    found_issues_this_scan = 0
    containers_scanned_count = 0
    error_message = None
    scan_timezone = pytz.timezone(SCHEDULER_TIMEZONE) # Get timezone object

    client = analyzer.get_docker_client()
    if not client:
        error_message = f"Failed to connect to Docker daemon at {datetime.now(scan_timezone).strftime('%Y-%m-%d %H:%M:%S %Z')}"
        scan_status["last_run_status"] = error_message
        scan_status["running"] = False
        # Optionally clear or mark existing statuses as stale? For now, keep old view.
        return

    try:
        running_containers = client.containers.list()
        containers_scanned_count = len(running_containers)
        logging.info(f"Found {containers_scanned_count} running containers.")

        if not running_containers:
             logging.info("No running containers found to scan.")
             # Clear the status if no containers are running
             with container_statuses_lock:
                container_statuses = {}
             # Add a placeholder to avoid empty state issues if needed
             # current_scan_results['placeholder'] = {'name': 'No Containers Found', 'status': 'info', 'details': None, 'db_id': None}

        for container in running_containers:
            container_id = container.id
            container_name = container.name
            logging.debug(f"Scanning container: {container_name} ({container_id[:12]})")

            # Add container with 'pending' status initially
            current_scan_results[container_id] = {
                'name': container_name,
                'id': container_id, # Add id here too for template convenience
                'status': 'pending',
                'details': None,
                'db_id': None # ID of the persistent abnormality record in DB
            }

            logs = analyzer.fetch_container_logs(container)
            if logs is None:
                current_scan_results[container_id]['status'] = 'error_fetching_logs'
                current_scan_results[container_id]['details'] = {'analysis': 'Failed to fetch logs', 'snippet': ''}
                continue

            analysis_result = analyzer.analyze_logs_with_ollama(logs)

            if analysis_result != "NORMAL" and not analysis_result.startswith("ERROR:"):
                found_issues_this_scan += 1
                logging.warning(f"Abnormality detected in {container_name}: {analysis_result}")
                log_snippet = analyzer.extract_log_snippet(analysis_result, logs)

                # Log/update persistent record in DB
                db.add_or_update_abnormality(container_name, container_id, log_snippet, analysis_result)

                # Find the corresponding DB ID for linking management actions
                db_id = db.get_latest_unresolved_abnormality_id(container_id, log_snippet)

                # Update in-memory status for the dashboard
                current_scan_results[container_id].update({
                    'status': 'unhealthy',
                    'details': {
                        'analysis': analysis_result,
                        'snippet': log_snippet,
                        'timestamp': datetime.now(scan_timezone) # Add timezone aware timestamp
                    },
                    'db_id': db_id
                })

            elif analysis_result.startswith("ERROR:"):
                 logging.error(f"Analysis error for {container_name}: {analysis_result}")
                 current_scan_results[container_id].update({
                    'status': 'error_analysis',
                    'details': {'analysis': analysis_result, 'snippet': '(Analysis failed)'},
                    'db_id': None
                 })
            else:
                 # Normal - Mark as healthy
                 current_scan_results[container_id]['status'] = 'healthy'
                 logging.info(f"Container {container_name} logs appear normal.")

        # --- Update Global State ---
        with container_statuses_lock:
            # Preserve order by sorting final results by name before assigning
            sorted_results = dict(sorted(current_scan_results.items(), key=lambda item: item[1]['name'].lower()))
            container_statuses = sorted_results # Replace old state with current results

        scan_status["last_run_status"] = (
            f"Completed at {datetime.now(scan_timezone).strftime('%Y-%m-%d %H:%M:%S %Z')}. "
            f"Scanned {containers_scanned_count} containers. "
            f"{found_issues_this_scan} unhealthy found/updated this scan."
        )
        logging.info(f"Scan finished. Updated status for {containers_scanned_count} containers.")

    except Exception as e:
        logging.exception("Unhandled error during scan task:")
        error_message = f"Error during scan at {datetime.now(scan_timezone).strftime('%Y-%m-%d %H:%M:%S %Z')}: {e}"
        scan_status["last_run_status"] = error_message
    finally:
        if client:
            client.close()
        scan_status["running"] = False
        # Update next run time (use scheduler's timezone)
        try:
            job = scheduler.get_job('docker_log_scan_job')
            if job and job.next_run_time:
                 scan_status["next_run_time"] = job.next_run_time # Keep timezone info
            else:
                 scan_status["next_run_time"] = None
        except Exception as e:
            logging.error(f"Could not get next run time: {e}")
            scan_status["next_run_time"] = None


# --- Flask Routes ---
@app.route('/')
def index():
    # Get a *copy* of the current statuses under lock
    with container_statuses_lock:
        current_statuses = container_statuses.copy() # Already sorted during update

    # Format next scan time for display
    next_run_time_str = "N/A"
    if scan_status["next_run_time"]:
         # Convert to local timezone for display if possible, else show original timezone
        try:
            # Try getting local system timezone if pytz is available
            import time
            local_tz_name = time.tzname[0] # Basic guess
            local_tz = pytz.timezone(local_tz_name)
            next_run_time_local = scan_status["next_run_time"].astimezone(local_tz)
            next_run_time_str = next_run_time_local.strftime('%Y-%m-%d %H:%M:%S %Z')
        except Exception: # Handle errors if timezone guess/conversion fails
            next_run_time_str = scan_status["next_run_time"].strftime('%Y-%m-%d %H:%M:%S %Z')

    return render_template('index.html',
                           container_statuses=current_statuses,
                           scan_status=scan_status["last_run_status"],
                           next_scan_time=next_run_time_str,
                           timezone=SCHEDULER_TIMEZONE) # Pass timezone name


# Route for managing specific abnormalities (can be linked from the main page)
@app.route('/manage/<int:abnormality_id>', methods=['GET', 'POST'])
def manage_abnormality(abnormality_id):
    abnormality = db.get_abnormality_by_id(abnormality_id)
    if not abnormality:
        flash(f'Abnormality with ID {abnormality_id} not found.', 'error')
        return redirect(url_for('index'))

    if request.method == 'POST':
        new_status = request.form.get('new_status')
        notes = request.form.get('notes', '').strip()

        if new_status not in ['resolved', 'ignored', 'unresolved']:
            flash('Invalid status provided.', 'error')
        else:
            success = db.update_abnormality_status(abnormality_id, new_status, notes if notes else None)
            if success:
                flash(f'Abnormality marked as {new_status}.', 'success')
                # Note: In-memory status will be updated by the next scan automatically if the underlying issue persists/resolves.
                return redirect(url_for('index')) # Redirect back to dashboard
            else:
                flash('Failed to update abnormality status.', 'error')
        # If POST fails validation or DB update, re-render the manage page
        return render_template('manage.html', abnormality=abnormality)

    # If GET request
    return render_template('manage.html', abnormality=abnormality)


# --- Scheduler Setup ---
scheduler = BackgroundScheduler(daemon=True, timezone=SCHEDULER_TIMEZONE)

def setup_scheduler():
    global SCHEDULER_TIMEZONE # Allow modification if invalid
    # Ensure pytz is available and timezone is valid
    try:
        pytz.timezone(SCHEDULER_TIMEZONE)
        logging.info(f"Using timezone '{SCHEDULER_TIMEZONE}' for scheduler.")
    except pytz.exceptions.UnknownTimeZoneError:
        logging.warning(f"Unknown timezone '{SCHEDULER_TIMEZONE}'. Defaulting to UTC.")
        SCHEDULER_TIMEZONE = "UTC"

    logging.info(f"Scheduling log scan job to run every {SCAN_INTERVAL_MINUTES} minutes.")
    scheduler.add_job(
        scan_docker_logs,
        trigger=IntervalTrigger(minutes=SCAN_INTERVAL_MINUTES),
        id='docker_log_scan_job',
        name='Docker Log Scan',
        replace_existing=True,
        next_run_time=datetime.now(pytz.timezone(SCHEDULER_TIMEZONE)) + timedelta(seconds=10) # Start soon
    )
    try:
        job = scheduler.get_job('docker_log_scan_job')
        if job and job.next_run_time:
             scan_status["next_run_time"] = job.next_run_time # Store timezone-aware datetime
    except Exception as e:
        logging.error(f"Could not get initial next run time: {e}")

    scheduler.start()
    logging.info("Scheduler started.")


# --- Main Execution ---
if __name__ == '__main__':
    # db.init_db() is called implicitly when db module is imported
    setup_scheduler() # Setup scheduler after ensuring timezone is valid
    logging.info(f"Ollama Endpoint: {OLLAMA_API_URL}")
    logging.info(f"Ollama Model: {OLLAMA_MODEL}")
    logging.info(f"Log Lines per Container: {LOG_LINES_TO_FETCH}")
    logging.info(f"Scan Interval: {SCAN_INTERVAL_MINUTES} minutes")
    # Use waitress or gunicorn in production instead of Flask's dev server
    # from waitress import serve
    # serve(app, host='0.0.0.0', port=5000)
    app.run(host='0.0.0.0', port=5000)
